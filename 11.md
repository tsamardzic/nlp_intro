## 11. Cross-lingual transfer and multilingual NLP

Explanations and visualisations 

> - Crash Course Linguistics [#16](https://youtu.be/-sUUWyo4RZQ)
> - Sebastian Ruder's blog: [The State of Multilingual AI](https://ruder.io/state-of-multilingual-ai/index.html)
> - [Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT](https://arxiv.org/pdf/1904.09077)
> - [The State and Fate of Linguistic Diversity and Inclusion in the NLP World](https://aclanthology.org/2020.acl-main.560.pdf) 
> - [Connecting Language Technologies with Rich, Diverse Data Sources Covering Thousands of Languages](https://aclanthology.org/2024.lrec-main.331.pdf)

&nbsp; 

<img src="figures/transfer-workflow-finetune.png" alt="finetune" width="520"/>


<img src="figures/transfer-workflow-continuepretrain.png" alt="continuepretrain" width="520"/>


<img src="figures/transfer-workflow-test.png" alt="test" width="320"/>


&nbsp; 

### Cross-lingual transfer

- In principle, we can pre-train a model on one language and use it to process texts in another language
- Most often, we pre-train models on multiple languages and then use them to process texts in any given single language
- *Target language* is the language in which we want to perform a task (process texts)
- **Double transfer**: when we transfer a model across languages and fine-tune it for a given task, there are two transfer steps, one across languages and one across tasks 
- **(Continued) training**: if we have some unlabelled data in the target language, we can continue training the model with the pre-training objective before fine-tuning it for a given task 
- *Zero-shot*: we attempt to perform a task without fine-tuning and continued training 
- An interesting example is the Helsinki team solution to the AmericasNLP task of translating from Spanish into low-resource languages: for each pari Spanish - Target, train a on English - Spanish for 90% of the time, then continue on Spanish - Target for 10% of the time, best results for all Target languages     
- Pre-trained model can be a **bare LM** or **trained/fine-tuned** for a specific task 

&nbsp; 

### Multilingual data sets 

- Universal Dependencies (UD) 106 languages, 20 families,  Bias towards Eurasia recognised but not intended
- Bible 100, 103 languages, 30 families, Majority non-Indo-European  
- mBERT, 97 languages, 15 families, Top 100 size of Wikipedia plus Thai and Mongolian
- XTREME,  40 languages, 14 families,  Diversity 
- XGLUE, 19 languages, 7 families 
- XNLI, 15 languages, 7 families, Span families, include low resource languages
- XCOPA,  11 languages, 11 families, Max diversity 
- TyDiQA, 11 languages, 10 families, Typological diversity
- XQuAD, 12 languages, 6 families, Extension to new languages


&nbsp; 

### Multilingual pre-trained models

BERT-type
- mBERT was the first, trained on top 100 Wikipedia languages, plus a few arbitrary ones
- XML-R, a BERT-based model, currently most popular as a starting point for multilingual experiments

GPT-type
- BLOOM
- NLLB

Full Transformers
- mT5 

Other pre-trained models are typically trained for a single language or a group of languages (e.g. Indic BERT, AraBERT, BERTiÄ‡)  


&nbsp; 


### Language similarity and transfer

&nbsp; 


<img src="figures/transfer-mentions.png" alt="mentions" width="620"/>


- If the target language is seen in pre-training, the performance will be better
- There is a trade-off between the size of the training data and the closeness to the target language 
- It is not easy to predict which will be good transfer-target pairs 
- Often BERT base (only English) works best even if the target language is very distant

&nbsp; 


### Language vectors

- One-hot encodings are used as language IDs   
- Typological feature values can be considered vectors, but actual features in the existing databases need to be processed, the two most common methods are conversion into binary values and interpolation (filling missing values).    
- Vectors learned from text samples are called language model (LM) vectors: typically a special token appended to each sentence (same token for all sentences of a single language), this token is expected to contain a representation of a language 
- Typological data bases: WALS, Glottolog, URIEL (derived from WALS, Glottolog and some other sources), Grambank


&nbsp; 


### Benefits of multilingual NLP

- Linguistic and machine learning: bigger challenges lead to better approaches, e.g. subword tokenisation  
- Cultural and normative: better representation of the real world knowledge
- Cognitive: learn interlingual abstractions  


&nbsp; 
